{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T13:55:19.731005Z",
     "iopub.status.busy": "2024-12-01T13:55:19.730447Z",
     "iopub.status.idle": "2024-12-01T13:55:19.734153Z",
     "shell.execute_reply": "2024-12-01T13:55:19.733680Z",
     "shell.execute_reply.started": "2024-12-01T13:55:19.730974Z"
    }
   },
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get install libsm6 libxext6 -y -qq\n",
    "# !apt-get install libgl1-mesa-glx graphviz -y -qq\n",
    "# !pip install -r requirements.txt -q\n",
    "# # !apt-get install unrar\n",
    "# import os\n",
    "# os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T13:55:21.930545Z",
     "iopub.status.busy": "2024-12-01T13:55:21.930000Z",
     "iopub.status.idle": "2024-12-01T13:55:26.540783Z",
     "shell.execute_reply": "2024-12-01T13:55:26.540261Z",
     "shell.execute_reply.started": "2024-12-01T13:55:21.930517Z"
    },
    "id": "GHwanagFXbag"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 13:55:22.507736: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-01 13:55:22.507806: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-01 13:55:22.509384: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 13:55:22.517915: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-01 13:55:23.754783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "time: 1.79 ms (started: 2024-12-01 13:55:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import PIL\n",
    "import numpy as np\n",
    "import cv2\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import natsort\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%load_ext autotime\n",
    "\n",
    "print(tf.__version__)\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.691860Z",
     "iopub.status.idle": "2024-12-01T07:08:44.692091Z",
     "shell.execute_reply": "2024-12-01T07:08:44.691996Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.691996Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install rarfile\n",
    "# import rarfile\n",
    "# def extract_rar(rar_file_path, extract_path):\n",
    "#     with rarfile.RarFile(rar_file_path, 'r') as rar_file:\n",
    "#         rar_file.extractall(extract_path)\n",
    "# rar_file_path = '/notebooks/Childrens dental segmentation dataset version 2.rar'\n",
    "# extract_path = '/notebooks/'\n",
    "# extract_rar(rar_file_path, extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.692574Z",
     "iopub.status.idle": "2024-12-01T07:08:44.692794Z",
     "shell.execute_reply": "2024-12-01T07:08:44.692706Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.692702Z"
    },
    "id": "QhWTi6rZxeXW"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH = 16\n",
    "ETA = 0.001\n",
    "WEIGHT_DECAY = 6e-8\n",
    "IMAGE_SHAPE = (256, 512, 1)\n",
    "MASK_SHAPE = (256, 512, 1)\n",
    "model_path = '/notebooks/saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.693573Z",
     "iopub.status.idle": "2024-12-01T07:08:44.693739Z",
     "shell.execute_reply": "2024-12-01T07:08:44.693668Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.693668Z"
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Enable GPU memory growth (optional)\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    # Set mixed precision policy\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"TensorFlow GPU device: \", tf.test.gpu_device_name())\n",
    "print(\"All devices: \", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ DATASET (IMAGES, MASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.694427Z",
     "iopub.status.idle": "2024-12-01T07:08:44.694606Z",
     "shell.execute_reply": "2024-12-01T07:08:44.694538Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.694538Z"
    },
    "id": "o9rbB3bdxeXX"
   },
   "outputs": [],
   "source": [
    "class readDataset:\n",
    "    def __init__(self, imagesPathes, masksPathes):\n",
    "        self.imagesPathes = imagesPathes\n",
    "        self.masksPathes = masksPathes\n",
    "        self.images = None\n",
    "        self.masks = None\n",
    "        self.val_images = None\n",
    "        self.val_masks = None\n",
    "        self.test_images = None\n",
    "        self.test_masks = None\n",
    "        \n",
    "    def readPathes(self,):\n",
    "        self.images = natsort.natsorted(list(pathlib.Path(self.imagesPathes).glob('*.*')))\n",
    "        self.masks = natsort.natsorted(list(pathlib.Path(self.masksPathes).glob('*.*')))\n",
    "        try:\n",
    "            shutil.rmtree(os.path.join(self.imagesPathes, \".ipynb_checkpoints\"))\n",
    "            shutil.rmtree(os.path.join(self.masksPathes, \".ipynb_checkpoints\"))\n",
    "            print(f\".ipynb_checkpoints directory deleted successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"just checking .ipynb_checkpoints (nothing)\")\n",
    "        \n",
    "    def readImages(self, data, typeData):\n",
    "        images = []\n",
    "        height = 256\n",
    "        width = 512\n",
    "        for img in data:\n",
    "            img_name = img.name\n",
    "            img = cv2.imread(str(img), 0)\n",
    "            img = cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)\n",
    "            if typeData == 'm':\n",
    "                img = np.where(img > 0, 1, 0)            \n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "            images.append(img)\n",
    "        print(\"(INFO..) Read Image Done\")\n",
    "        return np.array(images)\n",
    "\n",
    "    def normalizeImages(self, images):\n",
    "        normalized_images = []\n",
    "        # epsilon = 1e-7\n",
    "        clahe = cv2.createCLAHE(clipLimit=1, tileGridSize=(3,3))\n",
    "        for img in images:\n",
    "            img = clahe.apply(img)\n",
    "            # img = cv2.convertScaleAbs(img)\n",
    "            # img = cv2.equalizeHist(img)\n",
    "            img = img.astype(np.float32)\n",
    "            \n",
    "            # x = np.log(1 + 0.1 * img)\n",
    "            # x = (exp_img - 1)  / (0.1 + epsilon) # Apply reverse logarithmic transformation #to find the darkest pixels \n",
    "            # filter_img = cv2.GaussianBlur(x, (3,3), 0)\n",
    "            # filter_img = filter_img.astype(np.uint8)\n",
    "            # img = np.expand_dims(img, axis=-1)\n",
    "            # img = np.round(0.5 * img + (1 - 0.5) * filter_img).astype(np.uint8)\n",
    "            # img = (np.power(img/float(np.max(img)), 1.1) * 255.0).astype(np.uint8) #gamma correction method\n",
    "            \n",
    "            #Single retinex\n",
    "            # img = np.log1p(img) - cv2.GaussianBlur(np.log1p(img), (0, 0), 1000) \n",
    "            # img = np.exp(img) - 1.0\n",
    "            \n",
    "            img = (img - np.min(img)) / (np.max(img) - np.min(img)) * 255 #scaler\n",
    "            # img = np.clip(img, 0, 255).astype(np.uint8)  #set all pixel to [0,255]\n",
    "            \n",
    "            img = img / 255.\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "            normalized_images.append(img)\n",
    "        print(\"(INFO..) Normalization Image Done\")\n",
    "        return np.array(normalized_images)\n",
    "            \n",
    "    def dataAugmentation(self, images, masks):\n",
    "        if len(images) != len(masks):\n",
    "            raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        imagesupdate = []\n",
    "        masksupdate = []\n",
    "        for image, mask in zip(images, masks):\n",
    "            for aug in range(2):\n",
    "                imageup = image\n",
    "                maskup = mask\n",
    "                if aug == 1:\n",
    "                    imageup = tf.image.flip_left_right(imageup)\n",
    "                    maskup = tf.image.flip_left_right(maskup)\n",
    "                imagesupdate.append(imageup), masksupdate.append(maskup)\n",
    "                \n",
    "        imagesupdate2 = []\n",
    "        masksupdate2 = []\n",
    "        for image, mask in zip(imagesupdate, masksupdate):\n",
    "            contrast_factor = tf.random.uniform([], 0.3, 3)\n",
    "            for aug in range(2):\n",
    "                imageup = image\n",
    "                maskup = mask\n",
    "                if aug == 1:\n",
    "                    imageup = tf.image.adjust_contrast(imageup, contrast_factor)\n",
    "                    maskup_np = tf.make_ndarray(tf.make_tensor_proto(maskup))\n",
    "                    maskup_np = maskup_np.copy()\n",
    "                    maskup = tf.convert_to_tensor(maskup_np, dtype=tf.float32)\n",
    "                imagesupdate2.append(imageup), masksupdate2.append(maskup)\n",
    "        \n",
    "        imagesupdate3 = []\n",
    "        masksupdate3 = []\n",
    "        for image, mask in zip(imagesupdate2, masksupdate2):\n",
    "            brightness_delta = tf.random.uniform([], -0.45, 0.55)\n",
    "            for aug in range(2):\n",
    "                imageup = image\n",
    "                maskup = mask\n",
    "                if aug == 1:\n",
    "                    imageup = tf.image.adjust_brightness(imageup, delta=brightness_delta)\n",
    "                    maskup_np = tf.make_ndarray(tf.make_tensor_proto(maskup))\n",
    "                    maskup_np = maskup_np.copy()\n",
    "                    maskup = tf.convert_to_tensor(maskup_np, dtype=tf.float32)\n",
    "                imagesupdate3.append(imageup), masksupdate3.append(maskup)\n",
    "    \n",
    "        print(\"(INFO..) Augmentation Image Done\")\n",
    "        return np.array(imagesupdate3), np.array(masksupdate3)\n",
    "    \n",
    "    def splitDataset(self, images, masks, val_size=0.1, test_size=0.1, random_state=42):\n",
    "        data = list(zip(images, masks))\n",
    "        train_data, test_data = train_test_split(data, test_size=(val_size + test_size), random_state=random_state)\n",
    "        val_data, test_data = train_test_split(test_data, test_size=(test_size / (val_size + test_size)), random_state=random_state)\n",
    "\n",
    "        train_images, train_masks = zip(*train_data)\n",
    "        val_images, val_masks = zip(*val_data)\n",
    "        test_images, test_masks = zip(*test_data)\n",
    "        print(\"(INFO..) Splitting Data Done\")\n",
    "        return np.array(train_images), np.array(train_masks), np.array(val_images), np.array(val_masks), np.array(test_images), np.array(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.695900Z",
     "iopub.status.idle": "2024-12-01T07:08:44.696045Z",
     "shell.execute_reply": "2024-12-01T07:08:44.696009Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.696009Z"
    },
    "id": "l3UQmNwvnnIm"
   },
   "outputs": [],
   "source": [
    "datasetObject = readDataset(\"/notebooks/Childrens dental segmentation dataset version 2/images\",\n",
    "                           \"/notebooks/Childrens dental segmentation dataset version 2/mask\")\n",
    "datasetObject.readPathes()\n",
    "images = datasetObject.readImages(datasetObject.images, 'i')\n",
    "masks = datasetObject.readImages(datasetObject.masks, 'm')\n",
    "\n",
    "trainImages, trainMasks, validImages, validMasks, testImages, testMasks = datasetObject.splitDataset(images=images, masks=masks, val_size=20, test_size=10)\n",
    "trainImages, trainMasks = datasetObject.dataAugmentation(trainImages, trainMasks)\n",
    "\n",
    "trainImages = datasetObject.normalizeImages(trainImages)\n",
    "validImages = datasetObject.normalizeImages(validImages)\n",
    "testImages = datasetObject.normalizeImages(testImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T07:08:44.696982Z",
     "iopub.status.busy": "2024-12-01T07:08:44.696841Z",
     "iopub.status.idle": "2024-12-01T07:08:44.720106Z",
     "shell.execute_reply": "2024-12-01T07:08:44.719288Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.696982Z"
    },
    "id": "YQS24ueH_cyq",
    "outputId": "021f9a89-79bf-4291-b4d3-6a8450c01166"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainMasks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainMasks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(\u001b[43mtrainMasks\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m validMasks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(validMasks, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m testMasks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(testMasks, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainMasks' is not defined"
     ]
    }
   ],
   "source": [
    "trainMasks = np.squeeze(trainMasks, axis=-1)\n",
    "validMasks = np.squeeze(validMasks, axis=-1)\n",
    "testMasks = np.squeeze(testMasks, axis=-1)\n",
    "\n",
    "print(f\"Data Train: {trainImages.shape}, {trainMasks.shape}\")\n",
    "print(f\"Data Validation: {validImages.shape}, {validMasks.shape}\")\n",
    "print(f\"Data Test: {testImages.shape}, {testMasks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.720499Z",
     "iopub.status.idle": "2024-12-01T07:08:44.720674Z",
     "shell.execute_reply": "2024-12-01T07:08:44.720619Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.720619Z"
    },
    "id": "0jtV4nBCpzrC",
    "outputId": "d0b2f585-a2ab-42bb-a82e-3abc8fdd4e90"
   },
   "outputs": [],
   "source": [
    "np.unique(trainMasks), np.min(trainImages), np.max(trainImages), np.min(trainMasks), np.max(trainMasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.721299Z",
     "iopub.status.idle": "2024-12-01T07:08:44.721431Z",
     "shell.execute_reply": "2024-12-01T07:08:44.721390Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.721379Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 16))\n",
    "gs = gridspec.GridSpec(20 // 2, 4, width_ratios=[1, 1, 1, 1])\n",
    "for i in range(20):\n",
    "    ax0 = plt.subplot(gs[i // 2, i % 2])\n",
    "    ax1 = plt.subplot(gs[i // 2, i % 2 + 2])\n",
    "    ax0.imshow(validImages[i], cmap='gray', aspect='auto')\n",
    "    ax1.imshow(validMasks[i], cmap='gray', aspect='auto')\n",
    "    ax0.set_title('Image')\n",
    "    ax1.set_title('Mask')\n",
    "    ax0.axis('off')\n",
    "    ax1.axis('off')\n",
    "    ax0.set_aspect('equal')\n",
    "    ax1.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.722020Z",
     "iopub.status.idle": "2024-12-01T07:08:44.722174Z",
     "shell.execute_reply": "2024-12-01T07:08:44.722112Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.722103Z"
    }
   },
   "outputs": [],
   "source": [
    "def dice_score(y_true, y_pred, threshold=0.5):\n",
    "    smooth = 1.\n",
    "    y_true_f = tf.cast(tf.reshape(y_true, [-1]), dtype=tf.float32)\n",
    "    y_pred_f = tf.cast(tf.reshape(y_pred >= threshold, [-1]), dtype=tf.float32)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * tf.reduce_sum(intersection) + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1. - dice_score(y_true, y_pred)\n",
    "    return tf.cast(loss, dtype=tf.float32)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)(y_true, y_pred)\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    return tf.cast(bce, dtype=tf.float32) + dice\n",
    "\n",
    "def bfce_dice_loss(y_true, y_pred):\n",
    "    bce = tf.keras.losses.BinaryFocalCrossentropy()(y_true, y_pred)\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    return tf.cast(bce, dtype=tf.float32) + dice\n",
    "\n",
    "def binaryIoU_loss(y_true, y_pred):\n",
    "    IoU =  tf.keras.metrics.BinaryIoU()(y_true, y_pred)\n",
    "    return 1 - IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposing a generative neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.722602Z",
     "iopub.status.idle": "2024-12-01T07:08:44.722785Z",
     "shell.execute_reply": "2024-12-01T07:08:44.722687Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.722687Z"
    },
    "id": "lcAH125ap8MD"
   },
   "outputs": [],
   "source": [
    "def convolution(input, filter, padding, strides, kernel, activation, conv_type, name_prefix=''):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    x = input\n",
    "    x = layers.Conv2D(filter, kernel_size=kernel, padding=padding, strides=strides, \n",
    "                      kernel_initializer=initializer, use_bias=False, name=f'{name_prefix}_conv1')(x)\n",
    "    x = layers.GroupNormalization(groups=filter, name=f'{name_prefix}_gn1')(x)\n",
    "    \n",
    "    if conv_type == 'decoder':\n",
    "        x = layers.Activation(activation, name=f'{name_prefix}_activation1')(x)\n",
    "        x = layers.Conv2D(filter*2, kernel_size=kernel, padding=padding, strides=strides, \n",
    "                          kernel_initializer=initializer, use_bias=False, name=f'{name_prefix}_conv2')(x)\n",
    "        x = layers.GroupNormalization(groups=filter*2, name=f'{name_prefix}_gn2')(x)\n",
    "        x = layers.Activation(activation, name=f'{name_prefix}_activation2')(x)\n",
    "        x = layers.Conv2D(filter, kernel_size=kernel, padding=padding, strides=strides, \n",
    "                          kernel_initializer=initializer, use_bias=False, name=f'{name_prefix}_conv3')(x)\n",
    "        x = layers.GroupNormalization(groups=filter, name=f'{name_prefix}_gn3')(x)\n",
    "    \n",
    "    x = layers.average([x, layers.Conv2D(filter, kernel_size=1, padding='same', strides=1, kernel_initializer=initializer, name=f'{name_prefix}_conv4')(input)])\n",
    "    x = layers.Activation(activation, name=f'{name_prefix}_activation3')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.723858Z",
     "iopub.status.idle": "2024-12-01T07:08:44.724042Z",
     "shell.execute_reply": "2024-12-01T07:08:44.723971Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.723965Z"
    },
    "id": "u6t7y6YxqAc6"
   },
   "outputs": [],
   "source": [
    "def encoder(input, filter, padding, strides, kernel, activation, name_prefix):\n",
    "    x = input\n",
    "    x = convolution(x, filter, padding, strides, kernel, activation, 'encoder', name_prefix=name_prefix)\n",
    "    downsample = layers.AveragePooling2D(pool_size=(2, 2), name=f'{name_prefix}_pool')(x)\n",
    "    return downsample, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.724794Z",
     "iopub.status.idle": "2024-12-01T07:08:44.724986Z",
     "shell.execute_reply": "2024-12-01T07:08:44.724913Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.724913Z"
    },
    "id": "v3l_YejcqDaj"
   },
   "outputs": [],
   "source": [
    "def decoder(input, filter, skip, padding, strides, kernel, activation, name_prefix, dropout=False):\n",
    "    x = input\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    x = layers.Conv2DTranspose(filter, padding = padding, kernel_size = kernel,\n",
    "                               activation=activation, strides = 2, kernel_initializer=initializer,\n",
    "                               use_bias=False, name=f'{name_prefix}_transpose')(x)\n",
    "    \n",
    "    if dropout:\n",
    "        x = layers.Dropout(0.5, name=f'{name_prefix}_dropout')(x)\n",
    "    x = layers.average([x, skip], name=f'{name_prefix}_skip_connection')\n",
    "    x = convolution(x, filter, padding, strides, kernel, activation, 'decoder', name_prefix=name_prefix)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.726171Z",
     "iopub.status.idle": "2024-12-01T07:08:44.726360Z",
     "shell.execute_reply": "2024-12-01T07:08:44.726300Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.726300Z"
    }
   },
   "outputs": [],
   "source": [
    "def bottleneck(input, filters_bottleneck, strides, kernel, activation, depth=6, mode='cascade', name_prefix=''):\n",
    "    dilated_layers = []\n",
    "    x = input\n",
    "    if mode == 'cascade':\n",
    "        for i in range(depth):\n",
    "            residual = x\n",
    "            x = layers.Conv2D(filters_bottleneck, kernel_size=kernel, strides=strides,\n",
    "                               activation=activation, padding='same', dilation_rate=2**i,\n",
    "                              kernel_initializer=tf.random_normal_initializer(0., 0.02), name=f'{name_prefix}_conv_{i+1}')(x)\n",
    "            dilated_layers.append(x)\n",
    "        x = layers.add(dilated_layers, name=f'{name_prefix}_dilated_layers')\n",
    "        x = layers.GroupNormalization(groups=filters_bottleneck, name=f'{name_prefix}_gn')(x)\n",
    "        return x\n",
    "    \n",
    "    elif mode == 'cascade_residual':\n",
    "        for i in range(depth):\n",
    "            residual = x  # Save the input for the residual connection\n",
    "            x = layers.Conv2D(filters_bottleneck, kernel_size=kernel, strides=strides,\n",
    "                              activation=activation, padding='same', dilation_rate=2**i,\n",
    "                              kernel_initializer=tf.random_normal_initializer(0., 0.02))(x)\n",
    "            dilated_layers.append(x)\n",
    "            if i % 2 == 0:  # Add residual connection only on even convolutional layers\n",
    "                # Add a 1x1 convolution to the residual connection to match shapes\n",
    "                residual = layers.Conv2D(filters_bottleneck, kernel_size=(1, 1), strides=strides,\n",
    "                                         padding='same', kernel_initializer=tf.random_normal_initializer(0., 0.02))(residual)\n",
    "                residual = layers.GroupNormalization(groups=filters_bottleneck)(residual)\n",
    "                # Add the residual connection\n",
    "                x = layers.add([x, residual])\n",
    "            \n",
    "        cascade_layer = layers.add(dilated_layers)\n",
    "        fusion = layers.add([cascade_layer, x])\n",
    "        fusion = layers.GroupNormalization(groups=filters_bottleneck)(fusion)\n",
    "        return fusion\n",
    "    \n",
    "    elif mode == 'cascade_residual_multidimension':\n",
    "        for i in range(depth):\n",
    "            residual = x \n",
    "            x = layers.Conv2D(filters_bottleneck, kernel_size=kernel, strides=strides,\n",
    "                               activation=activation, padding='same', dilation_rate=2**i,\n",
    "                              kernel_initializer=tf.random_normal_initializer(0., 0.02))(x)\n",
    "            dilated_layers.append(x)\n",
    "            residual = layers.Conv2D(filters_bottleneck, kernel_size=(1, 1), strides=strides,\n",
    "                                     padding='same', kernel_initializer=tf.random_normal_initializer(0., 0.02))(residual)\n",
    "            residual = layers.GroupNormalization(groups=filters_bottleneck)(residual)\n",
    "            x = layers.add([x, residual])\n",
    "            dilated_layers.append(x)\n",
    "        fusion = layers.add(dilated_layers)\n",
    "        fusion = layers.GroupNormalization(groups=filters_bottleneck)(fusion)\n",
    "        return fusion\n",
    "    \n",
    "    elif mode == 'parallel':  # Like \"Atrous Spatial Pyramid Pooling\"\n",
    "        for i in range(depth):\n",
    "            dilated_layers.append(\n",
    "                layers.Conv2D(filters_bottleneck, kernel_size=kernel,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x)\n",
    "            )\n",
    "        parallel = layers.add(dilated_layers) \n",
    "        parallel = layers.GroupNormalization(groups=filters_bottleneck)(parallel)\n",
    "        return parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T07:08:44.731490Z",
     "iopub.status.busy": "2024-12-01T07:08:44.731307Z",
     "iopub.status.idle": "2024-12-01T07:08:44.737131Z",
     "shell.execute_reply": "2024-12-01T07:08:44.736587Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.731455Z"
    },
    "id": "X-01v1Ig_npR"
   },
   "outputs": [],
   "source": [
    "def generator(input, filter, padding, strides, kernel, model_weights, weights_path=''):\n",
    "    x = input\n",
    "    con1, skip1 = encoder(x, filter, padding=padding, strides=strides, kernel=kernel, activation='swish', name_prefix='encoder1')\n",
    "    con2, skip2 = encoder(con1, filter*2, padding=padding, strides=strides, kernel=kernel, activation='swish', name_prefix='encoder2')\n",
    "    con3, skip3 = encoder(con2, filter*4, padding=padding, strides=strides, kernel=kernel, activation='swish', name_prefix='encoder3')\n",
    "    con4, skip4 = encoder(con3, filter*8, padding=padding, strides=strides, kernel=kernel, activation='swish', name_prefix='encoder4')\n",
    "    # con5, skip5 = encoder(con4, filter*16, padding=padding, strides=strides, kernel=kernel, activation='swish', name_prefix='encoder5')\n",
    "    # con6, skip6 = encoder(con5, filter*16, padding=padding, strides=strides, kernel=kernel, activation='swish', name_prefix='encoder6')\n",
    "\n",
    "    # bottle = bottleneck(con6, filters_bottleneck=filter*2**5, strides=strides, kernel=kernel, \n",
    "    #                     activation='swish', depth=6, mode='cascade', name_prefix='bottleneck')\n",
    "    \n",
    "    # deco1 = decoder(bottle, filter*16, skip6, padding=padding, strides=strides, kernel=kernel, activation=layers.LeakyReLU(0.2), dropout=True, name_prefix='decoder1')\n",
    "    # deco1 = decoder(con5, filter*16, skip5, padding=padding, strides=strides, kernel=kernel, activation=layers.LeakyReLU(0.2), dropout=True, name_prefix='decoder1')\n",
    "    deco1 = decoder(con4, filter*8, skip4, padding=padding, strides=strides, kernel=kernel, activation=layers.LeakyReLU(0.2), name_prefix='decoder1')\n",
    "    deco2 = decoder(deco1, filter*4, skip3, padding=padding, strides=strides, kernel=kernel, activation=layers.LeakyReLU(0.2), name_prefix='decoder2')\n",
    "    deco3 = decoder(deco2, filter*2, skip2, padding=padding, strides=strides, kernel=kernel, activation=layers.LeakyReLU(0.2), name_prefix='decoder3')\n",
    "    deco4 = decoder(deco3, filter, skip1, padding=padding, strides=strides, kernel=kernel, activation=layers.LeakyReLU(0.2), name_prefix='decoder4')\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    output = layers.Conv2DTranspose(1, kernel_size=kernel, strides=strides, padding=padding, \n",
    "                                    kernel_initializer=initializer, activation='sigmoid', \n",
    "                                    name='generator_output_layer')(deco4)\n",
    "    \n",
    "    generator = models.Model(inputs=input, outputs=output, name='generator')\n",
    "    \n",
    "    if model_weights is not None:\n",
    "        generator.load_weights(weights_path)\n",
    "    generator.summary()\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T07:08:44.738164Z",
     "iopub.status.busy": "2024-12-01T07:08:44.738002Z",
     "iopub.status.idle": "2024-12-01T07:08:44.755727Z",
     "shell.execute_reply": "2024-12-01T07:08:44.754841Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.738164Z"
    },
    "id": "CfaTJ8qcAthx",
    "outputId": "d337ac27-e22e-42fa-e2b0-508657f0f94c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generator \u001b[38;5;241m=\u001b[39m generator(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mlayers\u001b[49m\u001b[38;5;241m.\u001b[39mInput(shape \u001b[38;5;241m=\u001b[39m IMAGE_SHAPE), \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      2\u001b[0m                             model_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m      3\u001b[0m                             weights_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "generator = generator(input=layers.Input(shape = IMAGE_SHAPE), filter=32, padding='same', kernel=3, strides=1, \n",
    "                            model_weights=None, \n",
    "                            weights_path=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.756119Z",
     "iopub.status.idle": "2024-12-01T07:08:44.756307Z",
     "shell.execute_reply": "2024-12-01T07:08:44.756259Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.756243Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(generator, show_shapes = True, show_layer_names=True, to_file='./chart/generator.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.756922Z",
     "iopub.status.idle": "2024-12-01T07:08:44.757107Z",
     "shell.execute_reply": "2024-12-01T07:08:44.757048Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.757048Z"
    },
    "id": "VqVGN_BRBEF5"
   },
   "outputs": [],
   "source": [
    "def discriminator(ImageInput, maskInput, filter, padding, strides, kernel, lossFn, learning_rate, weight_decay, model_weights, weights_path=''):\n",
    "    x = layers.multiply([ImageInput, maskInput])\n",
    "    con1, skip1 = encoder(x, filter, padding=padding, strides=strides, kernel=kernel, activation='swish', name_prefix='disc_encoder1')\n",
    "    con2, skip2 = encoder(con1, filter*2, padding=padding, strides=strides, kernel=kernel, activation='swish', name_prefix='disc_encoder2')\n",
    "    con3, skip3 = encoder(con2, filter*4, padding=padding, strides=strides, kernel=kernel, activation='swish', name_prefix='disc_encoder3')\n",
    "    con4, skip4 = encoder(con3, filter*8, padding=padding, strides=strides, kernel=kernel, activation='swish', name_prefix='disc_encoder4')\n",
    "    # con5, skip5 = encoder(con4, filter*16, padding=padding, strides=strides, kernel=kernel, activation='swish', name_prefix='disc_encoder5')\n",
    "    x = layers.GlobalAveragePooling2D()(con4)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    discriminator = models.Model(inputs=[maskInput, ImageInput], outputs=x, name='discriminator')\n",
    "    if model_weights is not None:\n",
    "        discriminator.load_weights(weights_path)\n",
    "    discriminator.compile(loss=lossFn, loss_weights=[1],\n",
    "                          optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, weight_decay=weight_decay),)\n",
    "    discriminator.summary()\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.757711Z",
     "iopub.status.idle": "2024-12-01T07:08:44.757890Z",
     "shell.execute_reply": "2024-12-01T07:08:44.757816Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.757816Z"
    },
    "id": "an1Idi66CkV4",
    "outputId": "7e5cdd46-6978-4956-c108-7c2236d0d122"
   },
   "outputs": [],
   "source": [
    "discriminator = discriminator(ImageInput=layers.Input(shape=IMAGE_SHAPE), \n",
    "                                    maskInput=layers.Input(shape=MASK_SHAPE),\n",
    "                                    filter=32, padding = 'same', strides=1, kernel=3, \n",
    "                                    lossFn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                                    learning_rate=ETA, weight_decay=WEIGHT_DECAY,\n",
    "                                    model_weights=None, \n",
    "                                    weights_path=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.758536Z",
     "iopub.status.idle": "2024-12-01T07:08:44.758700Z",
     "shell.execute_reply": "2024-12-01T07:08:44.758664Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.758664Z"
    },
    "id": "aFb-6rXOGWOQ"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(discriminator, show_shapes = True, show_layer_names=True, to_file='./chart/discriminator.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.759290Z",
     "iopub.status.idle": "2024-12-01T07:08:44.759469Z",
     "shell.execute_reply": "2024-12-01T07:08:44.759409Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.759409Z"
    },
    "id": "qSLaj3Ovqb9C"
   },
   "outputs": [],
   "source": [
    "def GAN(discriminator, generator, learning_rate, weight_decay, lossFn):\n",
    "    imageInput=layers.Input(shape=IMAGE_SHAPE, name=\"image_Input\")\n",
    "    maskInput=layers.Input(shape=MASK_SHAPE, name=\"mask_Input\")\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    fakeMasks = generator(imageInput)\n",
    "    discriminatorInput = discriminator([fakeMasks, imageInput])\n",
    "    \n",
    "    gan = models.Model(inputs=[maskInput, imageInput],\n",
    "                       outputs=[discriminatorInput, fakeMasks],\n",
    "                       name='cgan')\n",
    "    \n",
    "    gan.compile(loss=lossFn, loss_weights=[1,100], \n",
    "                optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate*0.5, weight_decay=weight_decay*0.5),\n",
    "                metrics=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "    gan.summary()\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.760022Z",
     "iopub.status.idle": "2024-12-01T07:08:44.760173Z",
     "shell.execute_reply": "2024-12-01T07:08:44.760148Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.760144Z"
    },
    "id": "AmAnNhVOEPGw",
    "outputId": "32d9d028-5a4f-476a-ed7f-1c168ea2d264"
   },
   "outputs": [],
   "source": [
    "gan = GAN(discriminator=discriminator, generator=generator,\n",
    "          learning_rate=ETA, weight_decay=WEIGHT_DECAY, \n",
    "          lossFn=[tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 'mae'])\n",
    "\n",
    "# Notes the loss functions is (BCE + MAE * 100(lambda))\n",
    "# Following the isola et al loss functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.760788Z",
     "iopub.status.idle": "2024-12-01T07:08:44.760939Z",
     "shell.execute_reply": "2024-12-01T07:08:44.760901Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.760901Z"
    },
    "id": "MxIt_xn0FcX3",
    "outputId": "d8c82dc6-695f-4bc5-ffee-3d7ca329334b"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(gan, show_shapes = True, show_layer_names=True, to_file='./chart/gan.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.761639Z",
     "iopub.status.idle": "2024-12-01T07:08:44.761771Z",
     "shell.execute_reply": "2024-12-01T07:08:44.761724Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.761724Z"
    },
    "id": "dBRZ5bekqhNi"
   },
   "outputs": [],
   "source": [
    "def samples(generator, images, realMasks):\n",
    "    predMasks = tf.squeeze(generator.predict(images, verbose=0))\n",
    "    plt.figure(figsize=(12,6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(3, 3, i * 3 + 1)\n",
    "        plt.title('Input Image')\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(3, 3, i * 3 + 2)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.imshow(realMasks[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(3, 3, i * 3 + 3)\n",
    "        plt.title('Predicted Mask')\n",
    "        plt.imshow(predMasks[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.762495Z",
     "iopub.status.idle": "2024-12-01T07:08:44.762622Z",
     "shell.execute_reply": "2024-12-01T07:08:44.762587Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.762570Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.763899Z",
     "iopub.status.idle": "2024-12-01T07:08:44.764042Z",
     "shell.execute_reply": "2024-12-01T07:08:44.764017Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.764001Z"
    },
    "id": "QhXyfVgZqlS5",
    "outputId": "a827b1ef-2e76-4936-f6fc-47033d789d0b"
   },
   "outputs": [],
   "source": [
    "dlossTagList = []\n",
    "glossTagList = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    indexs = np.random.randint(0, len(trainImages), size = (BATCH, ))\n",
    "    realImages = trainImages[indexs]\n",
    "    realMasks = trainMasks[indexs]\n",
    "    realTag = tf.ones(shape = (BATCH, ))\n",
    "\n",
    "    fakeMasks = tf.squeeze(generator.predict([realImages], verbose=0))    \n",
    "    fakeTag = tf.zeros(shape = (BATCH, ))\n",
    "    \n",
    "    allTags = np.hstack([realTag, fakeTag])\n",
    "    allMasks = np.vstack([realMasks, fakeMasks])\n",
    "    allImages = np.vstack([realImages, realImages])\n",
    "    dlossTag = discriminator.train_on_batch([allMasks, allImages], [allTags])\n",
    "    \n",
    "    glossTag = gan.train_on_batch([realMasks, realImages], [realTag, realMasks])\n",
    "    \n",
    "    dlossTagList.append(dlossTag)\n",
    "    glossTagList.append(glossTag) \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch/Epochs: {epoch}/{EPOCHS}\")\n",
    "        print(f\"discriminator loss: [tag: {dlossTag}], generator loss: [tag: {glossTag}]\")\n",
    "        validIndexs = np.random.randint(0, len(validImages), size = (3, ))\n",
    "        samples(generator, validImages[validIndexs], validMasks[validIndexs])  \n",
    "    elif epoch == EPOCHS-1:\n",
    "        print(f\"Epoch/Epochs: {epoch}/{EPOCHS}\")\n",
    "        print(f\"discriminator loss: [tag: {dlossTag}], generator loss: [tag: {glossTag}]\")\n",
    "        validIndexs = np.random.randint(0, len(validImages), size = (3, ))\n",
    "        samples(generator, validImages[validIndexs], validMasks[validIndexs])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T07:08:44.765736Z",
     "iopub.status.busy": "2024-12-01T07:08:44.765587Z",
     "iopub.status.idle": "2024-12-01T07:08:44.787308Z",
     "shell.execute_reply": "2024-12-01T07:08:44.786142Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.765684Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glossTagList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mglossTagList\u001b[49m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss 1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss 2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss 3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss 4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss 5\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss 1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss 1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss 5\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin mae index-: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss 1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39midxmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glossTagList' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(glossTagList, columns=[\"loss 1\", \"loss 2\", \"loss 3\", \"loss 4\", \"loss 5\"])\n",
    "df['loss 1'] = df['loss 1'] - df['loss 5']\n",
    "print(f\"min mae index-: {df['loss 1'].idxmin()}\")\n",
    "print(f\"min mae: {round(np.min(df['loss 1']),4)}\")\n",
    "print(f\"last mae: {round(df['loss 1'].iloc[-1],4)}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dlossTagList, label='Discriminator Loss (Tag)')\n",
    "plt.plot(df['loss 5'], label='Generator Loss (Tag)')\n",
    "plt.plot(df['loss 1'], label='L1 Loss (mae)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GAN Training Losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('/notebooks/chart/first_loss_chart.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.787931Z",
     "iopub.status.idle": "2024-12-01T07:08:44.788167Z",
     "shell.execute_reply": "2024-12-01T07:08:44.788096Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.788069Z"
    },
    "id": "IMMfbd5pDqpx"
   },
   "outputs": [],
   "source": [
    "generator_saved = f'4block_RMSprop_swish+leakyrelu_CGAN_{EPOCHS}_clahe_generator.h5'\n",
    "discriminator_saved = f'4block_RMSprop_swish+leakyrelu_CGAN_{EPOCHS}_clahe_discriminator.h5'\n",
    "\n",
    "generator_path = os.path.join(model_path, generator_saved)\n",
    "discriminator_path = os.path.join(model_path, discriminator_saved)\n",
    "\n",
    "generator.save_weights(generator_path)\n",
    "discriminator.save_weights(discriminator_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we will train the generator again with a number of layers frozen in order to maintain the capacity that the generator has gained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.788959Z",
     "iopub.status.idle": "2024-12-01T07:08:44.789183Z",
     "shell.execute_reply": "2024-12-01T07:08:44.789103Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.789058Z"
    },
    "id": "xV-CfhtkiSXy",
    "outputId": "09ccc349-fa33-47a1-8b3c-e727691a9169"
   },
   "outputs": [],
   "source": [
    "weight_path = generator_path\n",
    "\n",
    "generator2 = generator(input=layers.Input(shape=IMAGE_SHAPE), filter=32, padding='same', kernel=3, strides=1, \n",
    "                             model_weights=True, weights_path=weight_path)\n",
    "\n",
    "for layer in generator2.layers[:20]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-5, weight_decay=1e-7)\n",
    "generator2.compile(loss=bce_dice_loss,\n",
    "                         optimizer=optimizer, \n",
    "                         metrics=[tf.keras.metrics.Precision(name = 'precision'),\n",
    "                                  tf.keras.metrics.Recall(name = 'recall'), \n",
    "                                  tf.keras.metrics.BinaryIoU(name = 'IoU'),\n",
    "                                  dice_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.789775Z",
     "iopub.status.idle": "2024-12-01T07:08:44.789957Z",
     "shell.execute_reply": "2024-12-01T07:08:44.789886Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.789886Z"
    }
   },
   "outputs": [],
   "source": [
    "generator2.evaluate(validImages, validMasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.790562Z",
     "iopub.status.idle": "2024-12-01T07:08:44.790672Z",
     "shell.execute_reply": "2024-12-01T07:08:44.790632Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.790632Z"
    }
   },
   "outputs": [],
   "source": [
    "generator2.evaluate(testImages, testMasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.791140Z",
     "iopub.status.idle": "2024-12-01T07:08:44.791290Z",
     "shell.execute_reply": "2024-12-01T07:08:44.791250Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.791250Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw(images, masks, y_pred):\n",
    "    plt.figure(figsize = (12, 24))\n",
    "    index = -1\n",
    "    n = np.random.randint(y_pred.shape[0])\n",
    "    for i in range(120):\n",
    "        plt.subplot(20, 6, (i + 1))\n",
    "        if index == -1:\n",
    "            plt.imshow(images[n], cmap = 'gray')\n",
    "            plt.title('Image')\n",
    "            plt.axis('off')\n",
    "            index = 0\n",
    "        elif index == 0:\n",
    "            plt.imshow(images[n], cmap = 'gray')\n",
    "            plt.imshow(masks[n], alpha = 0.6, cmap = 'gray')\n",
    "            plt.title('Original Mask')\n",
    "            plt.axis('off')\n",
    "            index = 1\n",
    "        elif index == 1:\n",
    "            plt.imshow(images[n], cmap = 'gray')\n",
    "            plt.imshow(np.reshape(y_pred[n], IMAGE_SHAPE[:2]), alpha = 0.6, cmap = 'gray')\n",
    "            plt.title('Predict Mask')\n",
    "            plt.axis('off')\n",
    "            index = -1\n",
    "            n = np.random.randint(y_pred.shape[0])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.791737Z",
     "iopub.status.idle": "2024-12-01T07:08:44.791895Z",
     "shell.execute_reply": "2024-12-01T07:08:44.791846Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.791846Z"
    }
   },
   "outputs": [],
   "source": [
    "masks_pred = generator2.predict(validImages, verbose=0)\n",
    "masks_pred = (masks_pred >= 0.5).astype('int')\n",
    "masks_pred.shape\n",
    "draw(validImages, validMasks, masks_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-training of the generator according to the pixel2pixel methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.792593Z",
     "iopub.status.idle": "2024-12-01T07:08:44.792752Z",
     "shell.execute_reply": "2024-12-01T07:08:44.792723Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.792716Z"
    },
    "id": "GXU2wFydjICg",
    "outputId": "a13848d3-528c-4d16-c986-ec32e864cbe3"
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=1e-4,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.2,           # Reduce the learning rate by a factor of 0.2\n",
    "    patience=3,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_delta=1e-4,       # minimum changed in monitor metrics\n",
    ")\n",
    "\n",
    "history = generator2.fit(trainImages, trainMasks, epochs=200, batch_size=8,\n",
    "                               validation_data=(validImages, validMasks), verbose=2, \n",
    "                               callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The results obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.793366Z",
     "iopub.status.idle": "2024-12-01T07:08:44.793551Z",
     "shell.execute_reply": "2024-12-01T07:08:44.793475Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.793475Z"
    },
    "id": "7dYpz7WRZtkE"
   },
   "outputs": [],
   "source": [
    "metrics = ['loss', 'precision', 'recall', 'IoU', 'dice_score']\n",
    "plt.figure(figsize = (15, 9))\n",
    "for i in range(5):\n",
    "    plt.subplot(2, 3, (i + 1))\n",
    "    plt.plot(history.history['{}'.format(metrics[i])], label = '{}'.format(metrics[i]))\n",
    "    plt.plot(history.history['val_{}'.format(metrics[i])], label = 'val_{}'.format(metrics[i]))\n",
    "    plt.title('{}'.format(metrics[i]))\n",
    "    plt.legend()\n",
    "plt.savefig('/notebooks/chart/secondtrain_history_chart_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:44.793861Z",
     "iopub.status.idle": "2024-12-01T07:08:44.794016Z",
     "shell.execute_reply": "2024-12-01T07:08:44.793972Z",
     "shell.execute_reply.started": "2024-12-01T07:08:44.793972Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics = ['precision', 'recall', 'IoU', 'dice_score']\n",
    "plt.figure(figsize = (12, 8))\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, (i + 1))\n",
    "    plt.plot(history.history['{}'.format(metrics[i])], label = '{}'.format(metrics[i]))\n",
    "    plt.plot(history.history['val_{}'.format(metrics[i])], label = 'val_{}'.format(metrics[i]))\n",
    "    plt.title('{}'.format(metrics[i]))\n",
    "    plt.legend()\n",
    "plt.savefig('/notebooks/chart/secondtrain_history_chart_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T07:08:45.011508Z",
     "iopub.status.busy": "2024-12-01T07:08:45.011236Z",
     "iopub.status.idle": "2024-12-01T07:08:45.030389Z",
     "shell.execute_reply": "2024-12-01T07:08:45.029510Z",
     "shell.execute_reply.started": "2024-12-01T07:08:45.011486Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerator2\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(validImages, validMasks)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generator2' is not defined"
     ]
    }
   ],
   "source": [
    "generator2.evaluate(validImages, validMasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:45.031175Z",
     "iopub.status.idle": "2024-12-01T07:08:45.031359Z",
     "shell.execute_reply": "2024-12-01T07:08:45.031277Z",
     "shell.execute_reply.started": "2024-12-01T07:08:45.031269Z"
    }
   },
   "outputs": [],
   "source": [
    "generator2.evaluate(testImages, testMasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:45.031852Z",
     "iopub.status.idle": "2024-12-01T07:08:45.032089Z",
     "shell.execute_reply": "2024-12-01T07:08:45.032014Z",
     "shell.execute_reply.started": "2024-12-01T07:08:45.032006Z"
    }
   },
   "outputs": [],
   "source": [
    "generator_path2 = os.path.join(model_path, 'final', f'final_{generator_saved}')\n",
    "generator2.save(generator_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:45.032967Z",
     "iopub.status.idle": "2024-12-01T07:08:45.033187Z",
     "shell.execute_reply": "2024-12-01T07:08:45.033109Z",
     "shell.execute_reply.started": "2024-12-01T07:08:45.033101Z"
    }
   },
   "outputs": [],
   "source": [
    "generator3 = tf.keras.models.load_model(generator_path2,  \n",
    "                                        custom_objects={'bce_dice_loss':bce_dice_loss,\n",
    "                                                        'dice_score': dice_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:45.033992Z",
     "iopub.status.idle": "2024-12-01T07:08:45.034152Z",
     "shell.execute_reply": "2024-12-01T07:08:45.034082Z",
     "shell.execute_reply.started": "2024-12-01T07:08:45.034075Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predIdxs = generator3.predict(validImages)\n",
    "predIdxs = (predIdxs >= 0.5).astype('int')\n",
    "\n",
    "validMasks_flat = validMasks.flatten()\n",
    "predIdxs_flat = predIdxs.flatten()\n",
    "\n",
    "print(\"validMasks shape:\", validMasks_flat.shape)\n",
    "print(\"predIdxs shape:\", predIdxs_flat.shape)\n",
    "\n",
    "class_names = [\"Object\", \"Background\"]\n",
    "\n",
    "print(classification_report(\n",
    "    validMasks_flat, predIdxs_flat,\n",
    "    target_names=class_names\n",
    "))\n",
    "\n",
    "conf_mat = confusion_matrix(validMasks_flat, predIdxs_flat)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "plt.savefig('/notebooks/chart/secondtrain_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:45.035070Z",
     "iopub.status.idle": "2024-12-01T07:08:45.035229Z",
     "shell.execute_reply": "2024-12-01T07:08:45.035164Z",
     "shell.execute_reply.started": "2024-12-01T07:08:45.035157Z"
    },
    "id": "-cvl9117EZI4",
    "outputId": "938772f2-2d17-43e8-a818-538d6c56c059"
   },
   "outputs": [],
   "source": [
    "masks_pred = generator3.predict(validImages, verbose=0)\n",
    "masks_pred = (masks_pred >= 0.5).astype('int')\n",
    "print(masks_pred.shape)\n",
    "draw(validImages, validMasks, masks_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T07:08:45.243791Z",
     "iopub.status.busy": "2024-12-01T07:08:45.243166Z",
     "iopub.status.idle": "2024-12-01T07:08:45.262234Z",
     "shell.execute_reply": "2024-12-01T07:08:45.261309Z",
     "shell.execute_reply.started": "2024-12-01T07:08:45.243768Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m masks_pred \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator3\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(testImages, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m masks_pred \u001b[38;5;241m=\u001b[39m (masks_pred \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(masks_pred\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generator3' is not defined"
     ]
    }
   ],
   "source": [
    "masks_pred = generator3.predict(testImages, verbose=0)\n",
    "masks_pred = (masks_pred >= 0.5).astype('int')\n",
    "print(masks_pred.shape)\n",
    "draw(testImages, testMasks, masks_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:45.262827Z",
     "iopub.status.idle": "2024-12-01T07:08:45.263057Z",
     "shell.execute_reply": "2024-12-01T07:08:45.262954Z",
     "shell.execute_reply.started": "2024-12-01T07:08:45.262943Z"
    }
   },
   "outputs": [],
   "source": [
    "masks_pred = generator3.predict(trainImages, verbose=0)\n",
    "masks_pred = (masks_pred >= 0.5).astype('int')\n",
    "print(masks_pred.shape)\n",
    "draw(trainImages, trainMasks, masks_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-01T07:08:45.264021Z",
     "iopub.status.idle": "2024-12-01T07:08:45.264243Z",
     "shell.execute_reply": "2024-12-01T07:08:45.264143Z",
     "shell.execute_reply.started": "2024-12-01T07:08:45.264133Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile,os\n",
    "folder_to_zip = os.path.join(os.getcwd(),'chart')\n",
    "zip_file_name =  os.path.splitext(generator_saved)[0].replace(\"_generator\", \"\") + '.zip'\n",
    "with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for foldername, subfolders, filenames in os.walk(folder_to_zip):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(foldername, filename)\n",
    "            arcname = os.path.relpath(file_path, folder_to_zip)\n",
    "            zipf.write(file_path, arcname)\n",
    "zipf.close()\n",
    "\n",
    "print(f\"{zip_file_name} success zipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3480288,
     "sourceId": 6079337,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30528,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
